\chapter{Introduction}

\paragraph{}This thesis will cover the uses of \acrfull{ai} and \acrfull{dl} models such as \acrfull{llm} in educational cybersecurity simulations including hacking challenges such as \acrfull{ctf} and \acrfull{koth}. This will include using \acrshort{llm}(s) to generate narrative content (backstory), provide realistic personas for students to interact with, and even generate randomized challenges such as unique malware for malware analysis challenges.

\paragraph{}Modern \acrshort{llm}s are typically made up of transformer variants such as \acrfull{gpt}. These replaced older language model types such as \acrfull{lstm} models which are a type of \acrfull{rnn}. Transformer models were introduced in the paper \textcite{vaswani_attention_2023} by engineers at Google. These had better scalability and performance than the existing \acrlong{rnn} based approaches. One of the key features of transformers is the \acrfull{mha} mechanism. In some newer models this has evolved into techniques such as \acrfull{mla} and \acrfull{gqa} which are less computationally intensive. This thesis will be mainly looking at these transfomer based \acrshort{llm}s, though other kinds of \acrshort{llm}s such as \acrfull{llada} based models could be used. \acrshort{llada} is a novel kind of \acrshort{llm} which makes use of diffusion similar to image generation models such as stable diffusion.

\paragraph{}Most \acrlong{dl} models are both trained and inferenced on \acrfull{GPU} using \acrfull{GPGPU} techniques using \acrfull{api} such as \acrfull{cuda}, \acrfull{opencl}, and Vulkan. Some are also run on \acrfull{NPU} which are processors specificalised on doing training and inferenceing for deep learning models. Before the advent of this technique \acrlong{dl} models were run on \acrfull{CPU} hardware instead. This project will involve running \acrlong{dl} on both \acrshort{CPU}s and \acrshort{GPU}s.